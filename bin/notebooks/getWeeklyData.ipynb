{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary Libraries\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "### These are the libraries we need for writing the data to s3\n",
    "### https://khandelwal-shekhar.medium.com/read-and-write-to-from-s3-using-python-boto3-and-pandas-s3fs-144341e23aa1\n",
    "import io   \n",
    "from io import StringIO\n",
    "import yaml\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_file path\n",
    "aws_api_key_path = r\"..\\\\config\\\\cred_config.yaml\"\n",
    "config_file_path = r\"..\\\\config\\\\config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(aws_api_key_path,'r') as aws_api_file:\n",
    "    aws_api_config = yaml.safe_load(aws_api_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_file_path,'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argumant for Daily Load and One Time load\n",
    "oneTime = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuring the AWS keys\n",
    "AWS_S3_BUCKET = aws_api_config.get('AWS_S3_BUCKET')\n",
    "AWS_ACCESS_KEY_ID = aws_api_config.get('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY  = aws_api_config.get('AWS_SECRET_ACCESS_KEY')\n",
    "REGION_NAME = aws_api_config.get('REGION_NAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Configs\n",
    "index_output_data_file_name = config.get('index_path_50')[1]\n",
    "index_output_detailed_file_name = config.get('index_path_50')[2]\n",
    "index_path = config.get('index_path_50')[0]\n",
    "excluded_keys_info = config.get('excluded_keys_info') #['companyOfficers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_csv(\n",
    "    f\"s3://{AWS_S3_BUCKET}/{index_output_data_file_name}\",\n",
    "    storage_options = {\n",
    "        \"key\" : AWS_ACCESS_KEY_ID,\n",
    "        \"secret\" : AWS_SECRET_ACCESS_KEY,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "script= [x+\".NS\" for x in stocks[\"Symbol\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Daily_Data_from_all_stocks = {x:yf.download([x], period='max',interval='1d',group_by='ticker') for x in script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "Weekly_Data_from_all_stocks = {x:yf.download([x], period='max',interval='1wk',group_by='ticker') for x in script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in Daily_Data_from_all_stocks:\n",
    "    Daily_Data_from_all_stocks[script].stack(level=0,future_stack=True).rename_axis(['Date', 'Ticker']).reset_index(level=0).to_csv(\n",
    "        f\"s3://{AWS_S3_BUCKET}/daily_fl/{script}.csv\",\n",
    "        index=False,\n",
    "        storage_options = {\n",
    "            \"key\": AWS_ACCESS_KEY_ID,\n",
    "            \"secret\" : AWS_SECRET_ACCESS_KEY,\n",
    "        }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in Weekly_Data_from_all_stocks:\n",
    "    Weekly_Data_from_all_stocks[script].stack(level=0,future_stack=True).rename_axis(['Date', 'Ticker']).reset_index(level=0).to_csv(\n",
    "        f\"s3://{AWS_S3_BUCKET}/weekly_fl/{script}.csv\",\n",
    "        index=False,\n",
    "        storage_options = {\n",
    "            \"key\": AWS_ACCESS_KEY_ID,\n",
    "            \"secret\" : AWS_SECRET_ACCESS_KEY,\n",
    "        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2024-11-05 00:00:00+0000', tz='UTC'), 'ADANIENT.NS')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Daily_Data_from_all_stocks['ADANIENT.NS'].stack(level=0,future_stack=True).index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat_b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
